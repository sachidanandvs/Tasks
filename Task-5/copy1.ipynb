{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7489493",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3758,
     "status": "ok",
     "timestamp": 1623100925729,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "b0ee175a",
    "outputId": "9e97229d-4a77-4f79-ba3d-43ef1d042158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21cf741",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6142,
     "status": "ok",
     "timestamp": 1623100931869,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "s6FiV0enfQJ4",
    "outputId": "8c82e15a-73a2-4467-cd68-c6f7c95cfbed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "!pip install transformers\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b19e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1623100932491,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "4a493865",
    "outputId": "b7a7a2b8-fdf8-48b0-fdda-065ddb8cc360"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_data = pd.read_csv(\"/content/drive/MyDrive/yelp_review_full_csv/augmented_train_1300.csv\",header = None ,index_col = False)\n",
    "test_data = pd.read_csv(\"/content/drive/MyDrive/yelp_review_full_csv/test.csv\",header = None)\n",
    "\n",
    "EDA = True\n",
    "\n",
    "if EDA:\n",
    "    #train_data = train_data.iloc[: , 1:]\n",
    "    train_data.columns = [2 , 0 , 1]\n",
    "    train_data[0][0] = 1\n",
    "    #train_data = train_data.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92992fd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1623100932492,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "PaRntwu3ggqL",
    "outputId": "10f1cd2c-3c02-43bb-f367-1308e60a39e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16115785",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1623100932492,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "5d12be20",
    "outputId": "44de3912-b8dd-4bd3-e4df-fda44958c429"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8965, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15a846",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1623100932493,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "5B4sJy2WJ3Su",
    "outputId": "182a3d27-5969-4636-bd58-d0d595b7f8b2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>dr goldberg offers everything i look for in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>dr goldberg offers everything i look for in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>dr goldberg and everything i look for in a gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>dr goldberg offers everything i look for in a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     2  0                                                  1\n",
       "0  NaN  1                                                  1\n",
       "1  0.0  5  dr goldberg offers everything i look for in a ...\n",
       "2  1.0  5  dr goldberg offers everything i look for in a ...\n",
       "3  2.0  5  dr goldberg and everything i look for in a gen...\n",
       "4  3.0  5  dr goldberg offers everything i look for in a ..."
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda38f1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1623100932493,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "d6d32387",
    "outputId": "646d9f6a-cbb0-4292-c557-2ca1ff9bc9e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5437"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_data[1].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc969a5",
   "metadata": {
    "id": "ed784797"
   },
   "outputs": [],
   "source": [
    "#x_,x_train,y_,y_train = train_test_split(train_data[1],train_data[0],test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe035b",
   "metadata": {
    "id": "25de7212"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e09501",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1746,
     "status": "ok",
     "timestamp": 1623100934233,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "5c47a14e",
    "outputId": "72cc0dba-87d1-41f3-bada-18f113d0b0a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "model, pretrained_weights = (transformers.DistilBertModel, 'distilbert-base-uncased')\n",
    "model = model.from_pretrained(pretrained_weights)\n",
    "\n",
    "#for child in model.children():\n",
    "#  for param in child.parameters():\n",
    "#      param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f6667",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1623100934234,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "a556f82d",
    "outputId": "69f2a1b5-32ad-47de-9a4a-24f24b5814e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  dr goldberg offers everything i look for in a general practitioner hes nice and easy to without being patronizing hes always on time in seeing his patients hes affiliated with a top notch hospital nyu which my have explained to me is very in case something happens and you need surgery and you can get referrals to see specialists without having to see him first really what more do you need im here trying think of any complaints i have about him but im really drawing a blank\n",
      "Tokenized:  ['dr', 'goldberg', 'offers', 'everything', 'i', 'look', 'for', 'in', 'a', 'general', 'practitioner', 'he', '##s', 'nice', 'and', 'easy', 'to', 'without', 'being', 'patron', '##izing', 'he', '##s', 'always', 'on', 'time', 'in', 'seeing', 'his', 'patients', 'he', '##s', 'affiliated', 'with', 'a', 'top', 'notch', 'hospital', 'nyu', 'which', 'my', 'have', 'explained', 'to', 'me', 'is', 'very', 'in', 'case', 'something', 'happens', 'and', 'you', 'need', 'surgery', 'and', 'you', 'can', 'get', 'refer', '##ral', '##s', 'to', 'see', 'specialists', 'without', 'having', 'to', 'see', 'him', 'first', 'really', 'what', 'more', 'do', 'you', 'need', 'im', 'here', 'trying', 'think', 'of', 'any', 'complaints', 'i', 'have', 'about', 'him', 'but', 'im', 'really', 'drawing', 'a', 'blank']\n",
      "Token IDs:  [2852, 18522, 4107, 2673, 1045, 2298, 2005, 1999, 1037, 2236, 18742, 2002, 2015, 3835, 1998, 3733, 2000, 2302, 2108, 9161, 6026, 2002, 2015, 2467, 2006, 2051, 1999, 3773, 2010, 5022, 2002, 2015, 6989, 2007, 1037, 2327, 18624, 2902, 27935, 2029, 2026, 2031, 4541, 2000, 2033, 2003, 2200, 1999, 2553, 2242, 6433, 1998, 2017, 2342, 5970, 1998, 2017, 2064, 2131, 6523, 7941, 2015, 2000, 2156, 15744, 2302, 2383, 2000, 2156, 2032, 2034, 2428, 2054, 2062, 2079, 2017, 2342, 10047, 2182, 2667, 2228, 1997, 2151, 10821, 1045, 2031, 2055, 2032, 2021, 10047, 2428, 5059, 1037, 8744]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', train_data[1][1])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(train_data[1][1]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2997ea",
   "metadata": {
    "id": "e87aafb8"
   },
   "outputs": [],
   "source": [
    "#x_token = x_train.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)[:300]))\n",
    "\n",
    "#padded = np.array([i + [0]*(300 -len(i)) for i in x_token.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6fa04",
   "metadata": {
    "id": "4e607ddb"
   },
   "outputs": [],
   "source": [
    "#attention_mask = np.where(padded != 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6a854",
   "metadata": {
    "id": "d6dfbe5b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b99d4",
   "metadata": {
    "id": "b5adfebf"
   },
   "outputs": [],
   "source": [
    "class Yelp_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        review = str(self.data[1][index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data[0][index], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_dataset = Yelp_Dataset(train_data, tokenizer,200)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,drop_last = True)\n",
    "\n",
    "test_dataset = Yelp_Dataset(test_data[0:5000], tokenizer,200)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True,drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf96275",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1623100934237,
     "user": {
      "displayName": "Sachidanand V S ep19b010",
      "photoUrl": "",
      "userId": "00473899269515277127"
     },
     "user_tz": -330
    },
    "id": "3f675d60",
    "outputId": "d46509bc-2053-48e7-c12d-ea8ffe68c48b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (distill_bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (out): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.distill_bert = model\n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "        self.out = torch.nn.Linear(768, 5)\n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        distilbert_output = self.distill_bert(ids, mask)\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        output_1 = self.drop(pooled_output)\n",
    "        output = self.out(output_1)\n",
    "        return output\n",
    "    \n",
    "model = DistillBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7a22d",
   "metadata": {
    "id": "e703f603"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419d746",
   "metadata": {
    "id": "WQ7a2Xq9khl8"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    _, predicted = torch.max(y_pred.data, 1)\n",
    "    total = y.size(0)\n",
    "    correct = (predicted == y).sum().item()\n",
    "    return correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4148b2",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cb0894e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 50] accuracy: 0.207\n",
      "[1, 100] accuracy: 0.228\n",
      "[1, 150] accuracy: 0.239\n",
      "[1, 200] accuracy: 0.252\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(5):\n",
    "    acc = 0\n",
    "    for i,data in enumerate(train_loader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = data['ids'].to(device)\n",
    "        attention_mask = data['mask'].to(device)\n",
    "        labels = data['targets'].to(device)\n",
    "        labels = labels - 1\n",
    "        outputs = model(input_ids,attention_mask)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc += accuracy(outputs,labels)\n",
    "        if i%50 == 49:\n",
    "            print('[%d, %d] accuracy: %.3f' %\n",
    "                  (epoch + 1, i + 1, acc / ((i+1)*16)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f746202f",
   "metadata": {
    "id": "9f735122"
   },
   "outputs": [],
   "source": [
    "\n",
    "acc = 0\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(test_loader,0):\n",
    "        model.eval()\n",
    "        input_ids = data['ids'].to(device)\n",
    "        attention_mask = data['mask'].to(device)\n",
    "        labels = data['targets'].to(device)\n",
    "        labels = labels - 1\n",
    "        outputs = model(input_ids,attention_mask)\n",
    "        acc += accuracy(outputs,labels)\n",
    "print(i)\n",
    "acc/((i+1)*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0eaa96",
   "metadata": {
    "id": "qIfELmG6fOAf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "copy.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('py36': conda)",
   "language": "python",
   "name": "python369jvsc74a57bd047a2385e02337c9da7f7e3138ce6736e525632c23d10436a8fc6b989d8de9769"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
